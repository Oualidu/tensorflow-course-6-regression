{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734da498-d202-478c-ac1e-6c0932d117d9",
   "metadata": {
    "id": "734da498-d202-478c-ac1e-6c0932d117d9"
   },
   "outputs": [],
   "source": [
    "# Based on the TensFlow course : https://www.tensorflow.org/tutorials/keras/regression\n",
    "# Modified by Mehdi Ammi, Univ. Paris 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27268802-e9e5-45db-b04d-5edae9b1f91f",
   "metadata": {
    "id": "27268802-e9e5-45db-b04d-5edae9b1f91f"
   },
   "source": [
    "# TensorFlow: Regression for Predictive Modeling\n",
    "\n",
    "In regression tasks, the goal is to forecast the output of a continuous variable, such as a price or a probability. This differs from classification problems, where the objective is to choose a class from a set of classes (for instance, identifying whether a picture shows an apple or an orange).\n",
    "\n",
    "This tutorial employs the renowned Auto MPG dataset to illustrate how to construct models to predict the fuel efficiency of cars from the late 1970s and early 1980s. You will provide the models with detailed information about numerous cars from that era, including attributes like cylinders, displacement, horsepower, and weight.\n",
    "\n",
    "We'll utilize the Keras API in this example. (Refer to the Keras tutorials and guides for more information.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3dc3f-0ee3-44ea-9e71-92bb176c4043",
   "metadata": {
    "id": "3bd3dc3f-0ee3-44ea-9e71-92bb176c4043"
   },
   "source": [
    "## Install packages & import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d630b5c5-7061-4044-92a9-37fe04bbccfe",
   "metadata": {
    "id": "d630b5c5-7061-4044-92a9-37fe04bbccfe"
   },
   "outputs": [],
   "source": [
    "# Install the seaborn library for data visualization.\n",
    "!pip install -q seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb7100-6203-4423-9383-34735b5b5c57",
   "metadata": {
    "id": "d0bb7100-6203-4423-9383-34735b5b5c57"
   },
   "outputs": [],
   "source": [
    "# Import libraries for plotting, numerical operations, data manipulation, and visualization.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set NumPy print options for better readability.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ac684-f2a4-46bf-8573-cab9cde95e35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "998ac684-f2a4-46bf-8573-cab9cde95e35",
    "outputId": "17ac23b3-f6c9-4d83-a08a-9a3fc9fd85ba"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow and Keras for building neural networks.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Print the TensorFlow version.\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af339243-a5ef-4823-9888-65d0d48604e8",
   "metadata": {
    "id": "af339243-a5ef-4823-9888-65d0d48604e8"
   },
   "source": [
    "## The Auto MPG dataset\n",
    "\n",
    "The dataset is available from the UCI Machine Learning Repository.\n",
    "\n",
    "### Get the data\n",
    "First download and import the dataset using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3ec22-1c30-4a04-a9fc-f8bb6097d236",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35c3ec22-1c30-4a04-a9fc-f8bb6097d236",
    "outputId": "6fb6f265-02ae-48f1-be85-0d8260a7e8af"
   },
   "outputs": [],
   "source": [
    "# URL of the dataset to be loaded.\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "\n",
    "# Define the column names for the dataset.\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "# Load the dataset from the URL, specifying column names, handling missing values, and removing comments.\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "# Create a copy of the dataset for further manipulation and analysis.\n",
    "dataset = raw_dataset.copy()\n",
    "\n",
    "# Display the last few rows of the dataset.\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a4701-1ac8-4c2f-bc6d-c8f18efe30b4",
   "metadata": {
    "id": "9d2a4701-1ac8-4c2f-bc6d-c8f18efe30b4"
   },
   "source": [
    "|index|MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model Year|Origin|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|393|27\\.0|4|140\\.0|86\\.0|2790\\.0|15\\.6|82|1|\n",
    "|394|44\\.0|4|97\\.0|52\\.0|2130\\.0|24\\.6|82|2|\n",
    "|395|32\\.0|4|135\\.0|84\\.0|2295\\.0|11\\.6|82|1|\n",
    "|396|28\\.0|4|120\\.0|79\\.0|2625\\.0|18\\.6|82|1|\n",
    "|397|31\\.0|4|119\\.0|82\\.0|2720\\.0|19\\.4|82|1|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1218ae0-06a0-4f59-b3f7-22fd75ac8297",
   "metadata": {
    "id": "e1218ae0-06a0-4f59-b3f7-22fd75ac8297"
   },
   "source": [
    "### Clean the data\n",
    "\n",
    "The dataset has a few missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780a97b-971c-4ff5-87ba-c15be295ec7f",
   "metadata": {
    "id": "7780a97b-971c-4ff5-87ba-c15be295ec7f"
   },
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c98997-5b5f-408c-ab4a-2f1dda36bc52",
   "metadata": {
    "id": "54c98997-5b5f-408c-ab4a-2f1dda36bc52"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e058000-e607-4d41-8a0a-c1eb6d165c02",
   "metadata": {
    "id": "1e058000-e607-4d41-8a0a-c1eb6d165c02"
   },
   "source": [
    "Drop those rows to keep this initial tutorial simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6b25e6-b161-4f68-9bdd-5148bd9a9714",
   "metadata": {
    "id": "cc6b25e6-b161-4f68-9bdd-5148bd9a9714"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50730fb3-2676-4a64-b11e-8e0222a4e1de",
   "metadata": {
    "id": "50730fb3-2676-4a64-b11e-8e0222a4e1de"
   },
   "source": [
    "The \"Origin\" column is categorical, not numeric. So the next step is to one-hot encode the values in the column with pd.get_dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0358f2e7-9a08-4bf0-9b57-f4db7976f35e",
   "metadata": {
    "id": "0358f2e7-9a08-4bf0-9b57-f4db7976f35e"
   },
   "outputs": [],
   "source": [
    "# Map the values in the 'Origin' column to country labels.\n",
    "dataset['Origin'] = dataset['Origin'].map({1: 'USA', 2: 'Europe', 3: 'Japan'})\n",
    "\n",
    "# Convert the 'Origin' column to indicator variables (one-hot encoding).\n",
    "dataset = pd.get_dummies(dataset, columns=['Origin'], prefix='', prefix_sep='')\n",
    "\n",
    "# Display the last few rows of the dataset.\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594cc69-ee35-4499-8125-24e8ccf2e982",
   "metadata": {
    "id": "0594cc69-ee35-4499-8125-24e8ccf2e982"
   },
   "source": [
    "|index|MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model Year|Europe|Japan|USA|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|393|27\\.0|4|140\\.0|86\\.0|2790\\.0|15\\.6|82|false|false|true|\n",
    "|394|44\\.0|4|97\\.0|52\\.0|2130\\.0|24\\.6|82|true|false|false|\n",
    "|395|32\\.0|4|135\\.0|84\\.0|2295\\.0|11\\.6|82|false|false|true|\n",
    "|396|28\\.0|4|120\\.0|79\\.0|2625\\.0|18\\.6|82|false|false|true|\n",
    "|397|31\\.0|4|119\\.0|82\\.0|2720\\.0|19\\.4|82|false|false|true|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ec2a4-2f6c-43d7-8782-5ed1c6932917",
   "metadata": {
    "id": "501ec2a4-2f6c-43d7-8782-5ed1c6932917"
   },
   "source": [
    "### Split the data into training and testing sets\n",
    "\n",
    "Now, split the dataset into a training set and a test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8917f-129a-4af1-b551-742748260b07",
   "metadata": {
    "id": "56b8917f-129a-4af1-b551-742748260b07"
   },
   "outputs": [],
   "source": [
    "# Sample 80% of the data to create the training dataset.\n",
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "\n",
    "# Use the remaining 20% of the data to create the testing dataset.\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da4649-814d-48ad-9e17-34153518e09e",
   "metadata": {
    "id": "28da4649-814d-48ad-9e17-34153518e09e"
   },
   "source": [
    "### Inspect the data\n",
    "\n",
    "Review the joint distribution of a few pairs of columns from the training set.\n",
    "\n",
    "The top row suggests that the fuel efficiency (MPG) is a function of all the other parameters. The other rows indicate they are functions of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46599f72-3a8c-464a-8ce4-750ec05077e9",
   "metadata": {
    "id": "46599f72-3a8c-464a-8ce4-750ec05077e9"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac4bda-1485-44e8-8847-fad0e0d2dabc",
   "metadata": {
    "id": "65ac4bda-1485-44e8-8847-fad0e0d2dabc"
   },
   "source": [
    "Let's also check the overall statistics. Note how each feature covers a very\n",
    "\n",
    "1.   Élément de liste\n",
    "2.   Élément de liste\n",
    "\n",
    "different range:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5746cf2-c6d2-47b6-83a4-297ca1d2259a",
   "metadata": {
    "id": "e5746cf2-c6d2-47b6-83a4-297ca1d2259a"
   },
   "source": [
    "train_dataset.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfa78cb-1a0e-4bae-a758-645b12689ddd",
   "metadata": {
    "id": "5bfa78cb-1a0e-4bae-a758-645b12689ddd"
   },
   "source": [
    "|index|count|mean|std|min|25%|50%|75%|max|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|MPG|318\\.0|23\\.590566037735847|7\\.913617162025714|10\\.0|17\\.125|22\\.75|29\\.0|46\\.6|\n",
    "|Cylinders|318\\.0|5\\.427672955974843|1\\.6829413919287102|3\\.0|4\\.0|4\\.0|6\\.0|8\\.0|\n",
    "|Displacement|318\\.0|193\\.06132075471697|103\\.8127417257744|70\\.0|100\\.25|151\\.0|259\\.5|455\\.0|\n",
    "|Horsepower|313\\.0|104\\.06709265175719|38\\.67466171160924|46\\.0|75\\.0|92\\.0|120\\.0|230\\.0|\n",
    "|Weight|318\\.0|2963\\.8238993710693|844\\.7498054897484|1613\\.0|2219\\.25|2792\\.5|3571\\.25|5140\\.0|\n",
    "|Acceleration|318\\.0|15\\.595911949685535|2\\.796282280384398|8\\.0|13\\.9|15\\.5|17\\.3|24\\.8|\n",
    "|Model Year|318\\.0|75\\.94654088050315|3\\.7052657537475624|70\\.0|73\\.0|76\\.0|79\\.0|82\\.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98dbf39-674a-49f2-81f8-473d0e6c3da8",
   "metadata": {
    "id": "b98dbf39-674a-49f2-81f8-473d0e6c3da8"
   },
   "source": [
    "### Split features from labels\n",
    "\n",
    "Separate the target value—the \"label\"—from the features. This label is the value that you will train the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5297f88-367c-4842-8516-8f01c8432730",
   "metadata": {
    "id": "f5297f88-367c-4842-8516-8f01c8432730"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the training and testing datasets to separate features and labels.\n",
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "# Remove the target variable 'MPG' from the features dataset and store it separately as labels.\n",
    "train_labels = train_features.pop('MPG')\n",
    "test_labels = test_features.pop('MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a02d641-70c7-42bb-b8e4-62b321bac5c8",
   "metadata": {
    "id": "9a02d641-70c7-42bb-b8e4-62b321bac5c8"
   },
   "source": [
    "### Normalization\n",
    "\n",
    "In the table of statistics it's easy to see how different the ranges of each feature are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdee1dd3-aa91-4bf8-99e2-c381716b9bf8",
   "metadata": {
    "id": "fdee1dd3-aa91-4bf8-99e2-c381716b9bf8"
   },
   "outputs": [],
   "source": [
    "train_dataset.describe().transpose()[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0463928f-b758-48b5-979e-f2ec1f9a4d87",
   "metadata": {
    "id": "0463928f-b758-48b5-979e-f2ec1f9a4d87"
   },
   "source": [
    "|index|mean|std|\n",
    "|---|---|---|\n",
    "|MPG|23\\.590566037735847|7\\.913617162025714|\n",
    "|Cylinders|5\\.427672955974843|1\\.6829413919287102|\n",
    "|Displacement|193\\.06132075471697|103\\.8127417257744|\n",
    "|Horsepower|104\\.06709265175719|38\\.67466171160924|\n",
    "|Weight|2963\\.8238993710693|844\\.7498054897484|\n",
    "|Acceleration|15\\.595911949685535|2\\.796282280384398|\n",
    "|Model Year|75\\.94654088050315|3\\.7052657537475624|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b96f3-4ed8-4e5d-8475-cdd5c139a3c3",
   "metadata": {
    "id": "697b96f3-4ed8-4e5d-8475-cdd5c139a3c3"
   },
   "source": [
    "It is good practice to normalize features that use different scales and ranges.\n",
    "\n",
    "One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n",
    "\n",
    "Although a model might converge without feature normalization, normalization makes training much more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078a7a23-eb21-4340-85aa-927656a2e2cc",
   "metadata": {
    "id": "078a7a23-eb21-4340-85aa-927656a2e2cc"
   },
   "source": [
    "### The Normalization layer\n",
    "\n",
    "The tf.keras.layers.Normalization is a clean and simple way to add feature normalization into your model.\n",
    "\n",
    "The first step is to create the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197e3f9f-9806-4005-ab11-4280101b6d48",
   "metadata": {
    "id": "197e3f9f-9806-4005-ab11-4280101b6d48"
   },
   "outputs": [],
   "source": [
    "normalizer = tf.keras.layers.Normalization(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4185de60-ca7d-4231-8977-f88c17d96475",
   "metadata": {
    "id": "4185de60-ca7d-4231-8977-f88c17d96475"
   },
   "source": [
    "Then, fit the state of the preprocessing layer to the data by calling Normalization.adapt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b75a9d-83a5-47ee-ab8e-b73dc76c1300",
   "metadata": {
    "id": "87b75a9d-83a5-47ee-ab8e-b73dc76c1300"
   },
   "outputs": [],
   "source": [
    "# Convert train_features to a NumPy array with dtype float32 before adapting the normalizer.\n",
    "train_features_array = np.array(train_features, dtype=np.float32)\n",
    "\n",
    "# Adapt the normalizer to the training features.\n",
    "normalizer.adapt(train_features_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218259d8-5cb8-4d3e-8433-0f07c2c4ce09",
   "metadata": {
    "id": "218259d8-5cb8-4d3e-8433-0f07c2c4ce09"
   },
   "source": [
    "Calculate the mean and variance, and store them in the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404d54a6-0ed5-4825-8988-a1106d4c5ab5",
   "metadata": {
    "id": "404d54a6-0ed5-4825-8988-a1106d4c5ab5"
   },
   "outputs": [],
   "source": [
    "print(normalizer.mean.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597af90-d1bb-47f6-8f94-244453b0e594",
   "metadata": {
    "id": "9597af90-d1bb-47f6-8f94-244453b0e594"
   },
   "source": [
    "When the layer is called, it returns the input data, with each feature independently normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae491b5-d321-46d2-8066-7e91423e3865",
   "metadata": {
    "tags": [],
    "id": "8ae491b5-d321-46d2-8066-7e91423e3865"
   },
   "outputs": [],
   "source": [
    "# Convert boolean columns to float (0.0 and 1.0)\n",
    "train_features = train_features.astype(float)\n",
    "\n",
    "# Extract the first row of the training features as a NumPy array with dtype float32.\n",
    "first = np.array(train_features[:1], dtype=np.float32)\n",
    "\n",
    "with np.printoptions(precision=2, suppress=True):\n",
    "  print('First example:', first)\n",
    "  print()\n",
    "  print('Normalized:', normalizer(first).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b5bb1-1dda-4780-9d9e-b3502f3a0c51",
   "metadata": {
    "id": "245b5bb1-1dda-4780-9d9e-b3502f3a0c51"
   },
   "source": [
    "## Linear regression\n",
    "\n",
    "Before building a deep neural network model, start with linear regression using one and several variables.\n",
    "\n",
    "### Linear regression with one variable\n",
    "Begin with a single-variable linear regression to predict 'MPG' from 'Horsepower'.\n",
    "\n",
    "Training a model with tf.keras typically starts by defining the model architecture. Use a tf.keras.Sequential model, which represents a sequence of steps.\n",
    "\n",
    "There are two steps in your single-variable linear regression model:\n",
    "\n",
    " - Normalize the 'Horsepower' input features using the tf.keras.layers.Normalization preprocessing layer.\n",
    " - Apply a linear transformation (y = mx + b) to produce 1 output using a linear layer (tf.keras.layers.Dense).\n",
    "\n",
    "The number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time.\n",
    "\n",
    "First, create a NumPy array made of the 'Horsepower' features. Then, instantiate the tf.keras.layers.Normalization and fit its state to the horsepower data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76f97ac-4fb4-4ce4-83a0-6ca023d976f8",
   "metadata": {
    "id": "c76f97ac-4fb4-4ce4-83a0-6ca023d976f8"
   },
   "outputs": [],
   "source": [
    "# Extract the 'Horsepower' column from the training features as a NumPy array.\n",
    "horsepower = np.array(train_features['Horsepower'])\n",
    "\n",
    "# Create a Normalization layer for normalizing the 'Horsepower' data.\n",
    "horsepower_normalizer = layers.Normalization(input_shape=[1,], axis=None)\n",
    "\n",
    "# Adapt the normalization layer to the 'Horsepower' data, calculating the mean and variance.\n",
    "horsepower_normalizer.adapt(horsepower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a8ef5-d289-4e13-bd5e-9686f017af6b",
   "metadata": {
    "id": "e99a8ef5-d289-4e13-bd5e-9686f017af6b"
   },
   "source": [
    "Build the Keras Sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9b311f-c1e2-4f10-b686-81bd4d08b7b0",
   "metadata": {
    "id": "cb9b311f-c1e2-4f10-b686-81bd4d08b7b0"
   },
   "outputs": [],
   "source": [
    "horsepower_model = tf.keras.Sequential([\n",
    "    horsepower_normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])\n",
    "\n",
    "horsepower_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07fe23-20f9-44b1-9e68-2b85aa316691",
   "metadata": {
    "id": "ff07fe23-20f9-44b1-9e68-2b85aa316691"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "981eee17-337c-48a6-a86c-2eaec8e34896",
   "metadata": {
    "id": "981eee17-337c-48a6-a86c-2eaec8e34896"
   },
   "source": [
    "horsepower_model.predict(horsepower[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b53c8a-0da0-4e08-9c5d-8e19411cc6cc",
   "metadata": {
    "id": "d2b53c8a-0da0-4e08-9c5d-8e19411cc6cc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b15dc5e-6d68-4b3f-8001-bf8618a4f91e",
   "metadata": {
    "id": "2b15dc5e-6d68-4b3f-8001-bf8618a4f91e"
   },
   "source": [
    "Once the model is built, configure the training procedure using the Keras Model.compile method. The most important arguments to compile are the loss and the optimizer, since these define what will be optimized (mean_absolute_error) and how (using the tf.keras.optimizers.Adam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4795f60-b9fb-46ea-9f52-002f36b13b9f",
   "metadata": {
    "id": "d4795f60-b9fb-46ea-9f52-002f36b13b9f"
   },
   "outputs": [],
   "source": [
    "horsepower_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d745086-abea-4362-95db-67629448e0ff",
   "metadata": {
    "id": "5d745086-abea-4362-95db-67629448e0ff"
   },
   "source": [
    "Use Keras Model.fit to execute the training for 100 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ef0cb-07c0-4db0-bd8a-9d13eb7de956",
   "metadata": {
    "id": "a88ef0cb-07c0-4db0-bd8a-9d13eb7de956"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = horsepower_model.fit(\n",
    "    train_features['Horsepower'],\n",
    "    train_labels,\n",
    "    epochs=100,\n",
    "    # Suppress logging.\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b546cf42-2c34-4d36-b2cc-758c11ecdca2",
   "metadata": {
    "id": "b546cf42-2c34-4d36-b2cc-758c11ecdca2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f7cc536-db05-4e9a-bc75-04cd724f0990",
   "metadata": {
    "id": "0f7cc536-db05-4e9a-bc75-04cd724f0990"
   },
   "source": [
    "Visualize the model's training progress using the stats stored in the history object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf5802-06a3-4277-915a-b953869d9dc0",
   "metadata": {
    "id": "ddcf5802-06a3-4277-915a-b953869d9dc0"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f752853-d765-4c2f-9227-8842b16a9de3",
   "metadata": {
    "id": "8f752853-d765-4c2f-9227-8842b16a9de3"
   },
   "source": [
    "|index|loss|val\\_loss|epoch|\n",
    "|---|---|---|---|\n",
    "|95|3\\.803338050842285|4\\.1811957359313965|95|\n",
    "|96|3\\.802067279815674|4\\.20026969909668|96|\n",
    "|97|3\\.803834915161133|4\\.194756507873535|97|\n",
    "|98|3\\.811049222946167|4\\.185152530670166|98|\n",
    "|99|3\\.8024814128875732|4\\.211414337158203|99|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a5bdaf-a5b3-4d7b-9958-ceb76dd7dcf4",
   "metadata": {
    "id": "75a5bdaf-a5b3-4d7b-9958-ceb76dd7dcf4"
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.ylim([0, 10])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Error [MPG]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a51cb-176b-40f4-b851-bd26569cccdd",
   "metadata": {
    "id": "df9a51cb-176b-40f4-b851-bd26569cccdd"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8810de8d-8787-4ba4-8fce-5b8136647be6",
   "metadata": {
    "id": "8810de8d-8787-4ba4-8fce-5b8136647be6"
   },
   "source": [
    "Collect the results on the test set for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a23af-9786-4a31-a04e-e90132c5d558",
   "metadata": {
    "id": "761a23af-9786-4a31-a04e-e90132c5d558"
   },
   "outputs": [],
   "source": [
    "test_results = {}\n",
    "\n",
    "test_results['horsepower_model'] = horsepower_model.evaluate(\n",
    "    test_features['Horsepower'],\n",
    "    test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24318e-e0f7-4c62-a025-3398b5785204",
   "metadata": {
    "id": "9e24318e-e0f7-4c62-a025-3398b5785204"
   },
   "source": [
    "Since this is a single variable regression, it's easy to view the model's predictions as a function of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c9093-8672-4a47-8037-754f90a454cc",
   "metadata": {
    "id": "146c9093-8672-4a47-8037-754f90a454cc"
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(0.0, 250, 251)\n",
    "y = horsepower_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5995fac-7f6b-41fa-a8c3-cfc5b20ef42a",
   "metadata": {
    "id": "d5995fac-7f6b-41fa-a8c3-cfc5b20ef42a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf4667-60d7-4e0c-a09f-d9c753f7daad",
   "metadata": {
    "id": "02bf4667-60d7-4e0c-a09f-d9c753f7daad"
   },
   "outputs": [],
   "source": [
    "def plot_horsepower(x, y):\n",
    "  plt.scatter(train_features['Horsepower'], train_labels, label='Data')\n",
    "  plt.plot(x, y, color='k', label='Predictions')\n",
    "  plt.xlabel('Horsepower')\n",
    "  plt.ylabel('MPG')\n",
    "  plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87129497-3bb2-4974-9ce9-84a753229291",
   "metadata": {
    "id": "87129497-3bb2-4974-9ce9-84a753229291"
   },
   "outputs": [],
   "source": [
    "plot_horsepower(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a50b07-5b19-4c1d-9ac5-bf6184ec63d4",
   "metadata": {
    "id": "52a50b07-5b19-4c1d-9ac5-bf6184ec63d4"
   },
   "source": [
    "### Linear regression with multiple inputs\n",
    "You can use an almost identical setup to make predictions based on multiple inputs. This model still does the same\n",
    "y=mx+b except that m is a matrix and x is a vector.\n",
    "\n",
    "Create a two-step Keras Sequential model again with the first layer being normalizer (tf.keras.layers.Normalization(axis=-1)) you defined earlier and adapted to the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87fe06-afaf-4c5f-a3ae-ed932e71c5a2",
   "metadata": {
    "id": "0f87fe06-afaf-4c5f-a3ae-ed932e71c5a2"
   },
   "outputs": [],
   "source": [
    "linear_model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    layers.Dense(units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96eff75b-1c02-4161-93d7-4dd3a67be83f",
   "metadata": {
    "id": "96eff75b-1c02-4161-93d7-4dd3a67be83f"
   },
   "source": [
    "When you call Model.predict on a batch of inputs, it produces units=1 outputs for each example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c30a38-18f5-403f-b637-4bea5f330cbe",
   "metadata": {
    "id": "f0c30a38-18f5-403f-b637-4bea5f330cbe"
   },
   "outputs": [],
   "source": [
    "linear_model.predict(train_features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e84a9-453b-44e6-b049-c36712600c57",
   "metadata": {
    "id": "1d6e84a9-453b-44e6-b049-c36712600c57"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0638061-9fbf-44a5-9607-e713d449e8fe",
   "metadata": {
    "id": "f0638061-9fbf-44a5-9607-e713d449e8fe"
   },
   "source": [
    "When you call the model, its weight matrices will be built—check that the kernel weights (the m in y=mx+b) have a shape of (9, 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f490177-aa24-4ba2-ba08-b84006da2a83",
   "metadata": {
    "id": "3f490177-aa24-4ba2-ba08-b84006da2a83"
   },
   "outputs": [],
   "source": [
    "linear_model.layers[1].kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba27524-8b53-4855-804a-a8c6bd35dfbb",
   "metadata": {
    "id": "cba27524-8b53-4855-804a-a8c6bd35dfbb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79fb9e83-247c-45c9-ab3c-d0de855b0ebf",
   "metadata": {
    "id": "79fb9e83-247c-45c9-ab3c-d0de855b0ebf"
   },
   "source": [
    "Configure the model with Keras Model.compile and train with Model.fit for 100 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7e6869-24c5-4f79-ab59-85e0f7ec6e3e",
   "metadata": {
    "id": "7f7e6869-24c5-4f79-ab59-85e0f7ec6e3e"
   },
   "outputs": [],
   "source": [
    "linear_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "    loss='mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6d299-fed0-4291-aefb-13a4c7d0f704",
   "metadata": {
    "id": "0cf6d299-fed0-4291-aefb-13a4c7d0f704"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = linear_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    epochs=100,\n",
    "    # Suppress logging.\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af1308d-a9b9-49c8-9864-907127c4fa66",
   "metadata": {
    "id": "0af1308d-a9b9-49c8-9864-907127c4fa66"
   },
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215995a6-d813-4746-90aa-aa49d11fd067",
   "metadata": {
    "id": "215995a6-d813-4746-90aa-aa49d11fd067"
   },
   "source": [
    "Collect the results on the test set for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb04a7a-42e8-4679-afdc-cb6fdd043bfc",
   "metadata": {
    "id": "7eb04a7a-42e8-4679-afdc-cb6fdd043bfc"
   },
   "outputs": [],
   "source": [
    "# Ensure all features are converted to float32 to be compatible with TensorFlow.\n",
    "test_features = test_features.astype(np.float32)\n",
    "\n",
    "# Evaluate the model on the test dataset and store the results.\n",
    "test_results['linear_model'] = linear_model.evaluate(test_features, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc81e5-ea1f-4e29-8f88-253a308d48ac",
   "metadata": {
    "id": "36cc81e5-ea1f-4e29-8f88-253a308d48ac"
   },
   "source": [
    "## Regression with a deep neural network (DNN)\n",
    "\n",
    "In the previous section, you implemented two linear models for single and multiple inputs.\n",
    "\n",
    "Here, you will implement single-input and multiple-input DNN models.\n",
    "\n",
    "The code is basically the same except the model is expanded to include some \"hidden\" non-linear layers. The name \"hidden\" here just means not directly connected to the inputs or outputs.\n",
    "\n",
    "These models will contain a few more layers than the linear model:\n",
    "\n",
    " - The normalization layer, as before (with horsepower_normalizer for a single-input model and normalizer for a multiple-input model).\n",
    " - Two hidden, non-linear, Dense layers with the ReLU (relu) activation function nonlinearity.\n",
    " - A linear Dense single-output layer.\n",
    "\n",
    "Both models will use the same training procedure, so the compile method is included in the build_and_compile_model function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd739fa-c20e-4197-8886-89fd6d9545a6",
   "metadata": {
    "id": "bdd739fa-c20e-4197-8886-89fd6d9545a6"
   },
   "outputs": [],
   "source": [
    "def build_and_compile_model(norm):\n",
    "  model = keras.Sequential([\n",
    "      norm,\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(64, activation='relu'),\n",
    "      layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mean_absolute_error',\n",
    "                optimizer=tf.keras.optimizers.Adam(0.001))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1abbac-86d8-4cd3-9f58-72c04ccba284",
   "metadata": {
    "id": "5a1abbac-86d8-4cd3-9f58-72c04ccba284"
   },
   "source": [
    "### Regression using a DNN and a single input\n",
    "\n",
    "Create a DNN model with only 'Horsepower' as input and horsepower_normalizer (defined earlier) as the normalization layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec03b62-9ef8-4f87-8e56-90cc91c559ac",
   "metadata": {
    "id": "6ec03b62-9ef8-4f87-8e56-90cc91c559ac"
   },
   "outputs": [],
   "source": [
    "dnn_horsepower_model = build_and_compile_model(horsepower_normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b446d8db-837b-4769-8914-f65cd80d388d",
   "metadata": {
    "id": "b446d8db-837b-4769-8914-f65cd80d388d"
   },
   "source": [
    "This model has quite a few more trainable parameters than the linear models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d76ba-e513-4d79-a779-28dba8eb279b",
   "metadata": {
    "id": "ad9d76ba-e513-4d79-a779-28dba8eb279b"
   },
   "outputs": [],
   "source": [
    "dnn_horsepower_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e718d8f3-44f6-4267-bedb-b8ecaa23fa4e",
   "metadata": {
    "tags": [],
    "id": "e718d8f3-44f6-4267-bedb-b8ecaa23fa4e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80d2ba64-c169-43c8-ab7b-5f0e4d54763f",
   "metadata": {
    "id": "80d2ba64-c169-43c8-ab7b-5f0e4d54763f"
   },
   "source": [
    "Train the model with Keras Model.fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87433887-e40e-4580-862c-9316d96e2fb3",
   "metadata": {
    "id": "87433887-e40e-4580-862c-9316d96e2fb3",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a3372679-1632-4c01-dfa1-333e8b25cc14"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = dnn_horsepower_model.fit(\n",
    "    train_features['Horsepower'],\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd7459-4674-42ae-b600-fea0407e91fd",
   "metadata": {
    "id": "a7cd7459-4674-42ae-b600-fea0407e91fd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cd0577b-5662-4b0e-8198-35d2bc9a3226",
   "metadata": {
    "id": "5cd0577b-5662-4b0e-8198-35d2bc9a3226"
   },
   "source": [
    "This model does slightly better than the linear single-input horsepower_model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d92cd0-52dd-49b1-8ad6-3556d3f8d9ba",
   "metadata": {
    "id": "c0d92cd0-52dd-49b1-8ad6-3556d3f8d9ba",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "outputId": "c7d11601-bddc-4431-bc6c-ab1151514f3b"
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e29016c-4995-4f1b-bbd6-fadbe960ee50",
   "metadata": {
    "id": "4e29016c-4995-4f1b-bbd6-fadbe960ee50"
   },
   "source": [
    "If you plot the predictions as a function of 'Horsepower', you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1357c65-6159-4118-ab44-d1dc1e49c156",
   "metadata": {
    "id": "c1357c65-6159-4118-ab44-d1dc1e49c156",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "04310d8f-f4d3-422d-8821-0dc51a50b3cf"
   },
   "outputs": [],
   "source": [
    "x = tf.linspace(0.0, 250, 251)\n",
    "y = dnn_horsepower_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21fb864-e7a1-4b8e-8b1f-1597628e73c4",
   "metadata": {
    "id": "b21fb864-e7a1-4b8e-8b1f-1597628e73c4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cc5c6-5ad7-4d00-b60e-83be39f9913e",
   "metadata": {
    "id": "5f7cc5c6-5ad7-4d00-b60e-83be39f9913e"
   },
   "outputs": [],
   "source": [
    "plot_horsepower(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1ecb6b-270a-4f93-b988-dc342ac9db80",
   "metadata": {
    "id": "cc1ecb6b-270a-4f93-b988-dc342ac9db80"
   },
   "source": [
    "Collect the results on the test set for later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec247487-e7f1-4446-a2f3-04fd5d57dc6d",
   "metadata": {
    "id": "ec247487-e7f1-4446-a2f3-04fd5d57dc6d"
   },
   "outputs": [],
   "source": [
    "test_results['dnn_horsepower_model'] = dnn_horsepower_model.evaluate(\n",
    "    test_features['Horsepower'], test_labels,\n",
    "    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c10ff1-a80d-4d7e-8358-09c8af7c10f0",
   "metadata": {
    "id": "93c10ff1-a80d-4d7e-8358-09c8af7c10f0"
   },
   "source": [
    "### Regression using a DNN and multiple inputs\n",
    "\n",
    "Repeat the previous process using all the inputs. The model's performance slightly improves on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb9baf-f635-4e51-87b4-252ead044159",
   "metadata": {
    "id": "5cbb9baf-f635-4e51-87b4-252ead044159"
   },
   "source": [
    "dnn_model = build_and_compile_model(normalizer)\n",
    "dnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099dfc13-a7c9-4ad5-b75b-c9c5797bd3f7",
   "metadata": {
    "id": "099dfc13-a7c9-4ad5-b75b-c9c5797bd3f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1efbca8c-1e60-4826-8111-381d20623508",
   "metadata": {
    "id": "1efbca8c-1e60-4826-8111-381d20623508"
   },
   "source": [
    "%%time\n",
    "history = dnn_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2001d9dd-ae81-48e7-92ea-3c33ab610447",
   "metadata": {
    "id": "2001d9dd-ae81-48e7-92ea-3c33ab610447"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = dnn_model.fit(\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    validation_split=0.2,\n",
    "    verbose=0, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada69223-de28-42cd-a73e-5e0fe143a11e",
   "metadata": {
    "id": "ada69223-de28-42cd-a73e-5e0fe143a11e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8de218e2-7226-4cab-85e5-da2585a0f417",
   "metadata": {
    "id": "8de218e2-7226-4cab-85e5-da2585a0f417"
   },
   "source": [
    "Collect the results on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adae792a-6447-44e8-b02a-7fda3265c4ad",
   "metadata": {
    "id": "adae792a-6447-44e8-b02a-7fda3265c4ad",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 146
    },
    "outputId": "cd093df6-0a78-4b2f-e55f-625e579e57dd"
   },
   "outputs": [],
   "source": [
    "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245969ae-f3c5-467d-9f57-1d83e53c155f",
   "metadata": {
    "id": "245969ae-f3c5-467d-9f57-1d83e53c155f"
   },
   "source": [
    "### Performance\n",
    "\n",
    "Since all models have been trained, you can review their test set performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b3ad4-7530-4dfa-a04e-5501051d3a63",
   "metadata": {
    "id": "f39b3ad4-7530-4dfa-a04e-5501051d3a63"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(test_results, index=['Mean absolute error [MPG]']).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d132a75-eedc-4ee0-97a1-8f25baf7642e",
   "metadata": {
    "id": "0d132a75-eedc-4ee0-97a1-8f25baf7642e"
   },
   "source": [
    "|index|Mean absolute error \\[MPG\\]|\n",
    "|---|---|\n",
    "|horsepower\\_model|3\\.651707172393799|\n",
    "|linear\\_model|2\\.481541156768799|\n",
    "|dnn\\_horsepower\\_model|2\\.9002833366394043|\n",
    "|dnn\\_model|1\\.683077335357666|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5c8fb-7be0-4224-b456-2f9487e972df",
   "metadata": {
    "id": "b6d5c8fb-7be0-4224-b456-2f9487e972df"
   },
   "source": [
    "These results match the validation error observed during training.\n",
    "\n",
    "### Make predictions\n",
    "\n",
    "You can now make predictions with the dnn_model on the test set using Keras Model.predict and review the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c7d44-41d1-4c6b-9e5c-b9bec20ba595",
   "metadata": {
    "id": "678c7d44-41d1-4c6b-9e5c-b9bec20ba595"
   },
   "outputs": [],
   "source": [
    "test_predictions = dnn_model.predict(test_features).flatten()\n",
    "\n",
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 50]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30665d5-c938-4751-874b-c228bd8f073e",
   "metadata": {
    "id": "f30665d5-c938-4751-874b-c228bd8f073e"
   },
   "source": [
    "It appears that the model predicts reasonably well.\n",
    "\n",
    "Now, check the error distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27bc532-dd4b-4162-a870-96cd8e95f27f",
   "metadata": {
    "id": "b27bc532-dd4b-4162-a870-96cd8e95f27f"
   },
   "outputs": [],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Prediction Error [MPG]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "vrZupa3PA58N"
   },
   "id": "vrZupa3PA58N",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6410b974-48fe-4432-b1e3-1ee997c3b7bf",
   "metadata": {
    "id": "6410b974-48fe-4432-b1e3-1ee997c3b7bf"
   },
   "source": [
    "![reg_plot-9.png](attachment:cc754a66-3408-44bd-977e-91f5579c1171.png)"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "8pkSS8ZAApRR"
   },
   "id": "8pkSS8ZAApRR",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cdc08ad1-8519-4780-978c-ed1468ec86e1",
   "metadata": {
    "id": "cdc08ad1-8519-4780-978c-ed1468ec86e1"
   },
   "source": [
    "If you're happy with the model, save it for later use with Model.save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6df2d6-c584-4a26-a87f-9602a4f8734e",
   "metadata": {
    "id": "fd6df2d6-c584-4a26-a87f-9602a4f8734e"
   },
   "outputs": [],
   "source": [
    "dnn_model.save('dnn_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e80eff0-cb55-4e7f-9fa7-a798e0383762",
   "metadata": {
    "id": "8e80eff0-cb55-4e7f-9fa7-a798e0383762"
   },
   "source": [
    "## Exercices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ba07ff-6026-42bc-808f-8a7eedf71343",
   "metadata": {
    "id": "48ba07ff-6026-42bc-808f-8a7eedf71343"
   },
   "source": [
    "### Exercise 1: Data Preprocessing\n",
    "\n",
    "1. Load and preprocess the Auto MPG dataset, including handling missing values and one-hot encoding categorical variables.\n",
    "2. Replace missing values in the 'Horsepower' column with the median value instead of dropping them.\n",
    "3. Modify the one-hot encoding to include a prefix for the origin countries ('Origin_').\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercice 1 — Data preprocessing\n",
    "\n",
    "# On repart du dataset brut\n",
    "dataset = raw_dataset.copy()\n",
    "\n",
    "# Remplacer les valeurs manquantes de Horsepower par la médiane\n",
    "mediane_hp = dataset[\"Horsepower\"].median()\n",
    "dataset[\"Horsepower\"] = dataset[\"Horsepower\"].fillna(mediane_hp)\n",
    "\n",
    "# One-hot encoding de Origin avec préfixe Origin_\n",
    "dataset = pd.get_dummies(dataset, columns=[\"Origin\"], prefix=\"Origin\")\n",
    "\n",
    "\n",
    "\n",
    "# Vérification\n",
    "dataset.isna().sum(), dataset.head()"
   ],
   "metadata": {
    "id": "nTvZtbfT-vLO"
   },
   "id": "nTvZtbfT-vLO",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "23b76bb6-6b8b-437e-8f96-4d21a50cd706",
   "metadata": {
    "id": "23b76bb6-6b8b-437e-8f96-4d21a50cd706"
   },
   "source": [
    "### Exercise 2: Single-Variable Linear Regression\n",
    "\n",
    "1. Create and train a single-variable linear regression model to predict 'MPG' from 'Horsepower'.\n",
    "2. Use 'Weight' as the single feature for prediction instead of 'Horsepower'.\n",
    "3. Change the optimizer from 'Adam' to 'SGD' with a learning rate of 0.01 and retrain the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 2: Single-Variable Linear Regression\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cible\n",
    "y = dataset[\"MPG\"]\n",
    "\n",
    "# 1) MPG depuis Horsepower (Adam)\n",
    "X = dataset[[\"Horsepower\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "modele_hp = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
    "modele_hp.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "modele_hp.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "print(\"MAE (Horsepower, Adam) =\", modele_hp.evaluate(X_test, y_test, verbose=0)[1])\n",
    "\n",
    "# 2) MPG depuis Weight (Adam)\n",
    "X = dataset[[\"Weight\"]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "modele_w = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
    "modele_w.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "modele_w.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "print(\"MAE (Weight, Adam) =\", modele_w.evaluate(X_test, y_test, verbose=0)[1])\n",
    "\n",
    "# 3) MPG depuis Weight (SGD lr=0.01) et ré-entrainement\n",
    "# (normalisation pour éviter NaN avec SGD)\n",
    "\n",
    "normaliseur = tf.keras.layers.Normalization()\n",
    "normaliseur.adapt(X_train.to_numpy())\n",
    "\n",
    "modele_w_sgd = tf.keras.Sequential([normaliseur, tf.keras.layers.Dense(1)])\n",
    "modele_w_sgd.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "modele_w_sgd.fit(X_train.to_numpy(), y_train.to_numpy(), epochs=100, verbose=0)\n",
    "print(\"MAE (Weight, SGD lr=0.01) =\",\n",
    "      modele_w_sgd.evaluate(X_test.to_numpy(), y_test.to_numpy(), verbose=0)[1])"
   ],
   "metadata": {
    "id": "0uNKG1et-vx3"
   },
   "id": "0uNKG1et-vx3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "66c95fc4-6004-43a5-b4df-169d35a646f7",
   "metadata": {
    "id": "66c95fc4-6004-43a5-b4df-169d35a646f7"
   },
   "source": [
    "### Exercise 3: Multi-Variable Linear Regression\n",
    "\n",
    "1. Create and train a linear regression model using multiple features.\n",
    "2. Add an additional Dense layer with 10 units before the output layer and retrain the model.\n",
    "3. Change the learning rate to 0.05 and retrain the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 3: Multi-Variable Linear Regression\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cible\n",
    "y = dataset[\"MPG\"]\n",
    "\n",
    "# Variables explicatives (toutes sauf MPG)\n",
    "X = dataset.drop(columns=[\"MPG\"])\n",
    "\n",
    "# Séparation train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 1 Régression linéaire multi-variables (Adam)\n",
    "\n",
    "modele_multi = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "modele_multi.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "modele_multi.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "print(\"MAE (multi-features, Adam) =\",\n",
    "      modele_multi.evaluate(X_test, y_test, verbose=0)[1])\n",
    "\n",
    "\n",
    "\n",
    "# 2 Ajout d'une couche Dense(10) avant la sortie\n",
    "modele_multi_dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "modele_multi_dense.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "modele_multi_dense.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "print(\"MAE (Dense 10 + sortie, Adam) =\",\n",
    "      modele_multi_dense.evaluate(X_test, y_test, verbose=0)[1])\n",
    "\n",
    "\n",
    "\n",
    "# 3) Même modèle avec learning rate = 0.05\n",
    "modele_multi_lr = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "modele_multi_lr.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.05),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "\n",
    "modele_multi_lr.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "print(\"MAE (Dense 10, Adam lr=0.05) =\",\n",
    "      modele_multi_lr.evaluate(X_test, y_test, verbose=0)[1])\n"
   ],
   "metadata": {
    "id": "Fm9DKOQG-wVj"
   },
   "id": "Fm9DKOQG-wVj",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "502fe4df-23f8-45aa-a2f7-455a5c57cd34",
   "metadata": {
    "id": "502fe4df-23f8-45aa-a2f7-455a5c57cd34"
   },
   "source": [
    "### Exercise 4: Deep Neural Network Regression\n",
    "\n",
    "1. Create and train a deep neural network model using multiple features.\n",
    "2. Use 3 hidden layers with 128, 64, and 32 units respectively.\n",
    "3. Change the activation function of the hidden layers from 'relu' to 'tanh' and retrain the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 4: Deep Neural Network Regression\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# cible\n",
    "y = dataset[\"MPG\"]\n",
    "\n",
    "# features (toutes sauf MPG)\n",
    "X = dataset.drop(columns=[\"MPG\"])\n",
    "\n",
    "# split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "#  Modèle DNN (relu) : 128, 64, 32\n",
    "dnn_relu = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\", input_shape=[X_train.shape[1]]),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "dnn_relu.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "dnn_relu.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "print(\"MAE (DNN relu) =\", dnn_relu.evaluate(X_test, y_test, verbose=0)[1])\n",
    "\n",
    "\n",
    "#  Même modèle mais activation = tanh\n",
    "dnn_tanh = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation=\"tanh\", input_shape=[X_train.shape[1]]),\n",
    "    tf.keras.layers.Dense(64, activation=\"tanh\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"tanh\"),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "dnn_tanh.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "dnn_tanh.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "\n",
    "print(\"MAE (DNN tanh) =\", dnn_tanh.evaluate(X_test, y_test, verbose=0)[1])"
   ],
   "metadata": {
    "id": "Q49qiVlF-wuj"
   },
   "id": "Q49qiVlF-wuj",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "51bb5632-d4a8-4333-8716-0299f6097076",
   "metadata": {
    "id": "51bb5632-d4a8-4333-8716-0299f6097076"
   },
   "source": [
    "### Exercise 5: Evaluating Model Performance\n",
    "\n",
    "1. Evaluate the single-variable linear regression model on the test dataset.\n",
    "2. Plot the true vs. predicted 'MPG' values for the test dataset.\n",
    "3. Compute and plot the distribution of prediction errors (true values - predicted values).\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 5: Evaluating Model Performance\n",
    "# (modèle 1 variable : MPG - Horsepower)\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# On reprend la même entrée que le modèle simple : Horsepower seulement\n",
    "X_hp = dataset[[\"Horsepower\"]]\n",
    "y = dataset[\"MPG\"]\n",
    "\n",
    "X_train_hp, X_test_hp, y_train_hp, y_test_hp = train_test_split(\n",
    "    X_hp, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 1 Évaluer le modèle sur le test\n",
    "mae_test = modele_hp.evaluate(X_test_hp, y_test_hp, verbose=0)[1]\n",
    "print(\"MAE sur le jeu de test =\", mae_test)\n",
    "\n",
    "# 2 Prédire sur le test\n",
    "y_pred = modele_hp.predict(X_test_hp, verbose=0).flatten()\n",
    "\n",
    "# 3) Plot vrai vs prédit\n",
    "plt.figure()\n",
    "plt.scatter(y_test_hp, y_pred)\n",
    "plt.xlabel(\"MPG réel\")\n",
    "plt.ylabel(\"MPG prédit\")\n",
    "plt.title(\"MPG réel vs MPG prédit (test)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 4 Erreurs (vrai - prédit) + histogramme\n",
    "erreurs = y_test_hp.to_numpy() - y_pred\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(erreurs, bins=30)\n",
    "plt.xlabel(\"Erreur (MPG réel - MPG prédit)\")\n",
    "plt.ylabel(\"Nombre\")\n",
    "plt.title(\"Distribution des erreurs (test)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "p_szWB0E-xPq"
   },
   "id": "p_szWB0E-xPq",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9ee83a10-bda2-486b-bbd7-104180e45e0b",
   "metadata": {
    "id": "9ee83a10-bda2-486b-bbd7-104180e45e0b"
   },
   "source": [
    "### Exercise 6: Feature Engineering\n",
    "\n",
    "1. Add polynomial features (e.g., square and cubic terms) for 'Horsepower' to the dataset and retrain the linear regression model.\n",
    "2. Implement feature scaling using Min-Max normalization instead of standard normalization and retrain the model.\n",
    "3. Compare the performance of the model with polynomial features to the original linear model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 6: Feature Engineering\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Cible\n",
    "y = dataset[\"MPG\"]\n",
    "\n",
    "\n",
    "\n",
    "# 1) Modèle linéaire original (Horsepower seul)\n",
    "\n",
    "X_lin = dataset[[\"Horsepower\"]]\n",
    "\n",
    "X_train_lin, X_test_lin, y_train, y_test = train_test_split(\n",
    "    X_lin, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "modele_lin = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "modele_lin.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "modele_lin.fit(X_train_lin, y_train, epochs=100, verbose=0)\n",
    "\n",
    "mae_lin = modele_lin.evaluate(X_test_lin, y_test, verbose=0)[1]\n",
    "print(\"MAE modèle linéaire =\", mae_lin)\n",
    "\n",
    "\n",
    "# 2 Ajout de features polynomiales (HP, HP², HP³)\n",
    "\n",
    "X_poly = pd.DataFrame({\n",
    "    \"HP\": dataset[\"Horsepower\"],\n",
    "    \"HP2\": dataset[\"Horsepower\"] ** 2,\n",
    "    \"HP3\": dataset[\"Horsepower\"] ** 3\n",
    "})\n",
    "\n",
    "X_train_poly, X_test_poly, y_train, y_test = train_test_split(\n",
    "    X_poly, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# 3) Min-Max normalization\n",
    "scaler = MinMaxScaler()\n",
    "X_train_poly = scaler.fit_transform(X_train_poly)\n",
    "X_test_poly = scaler.transform(X_test_poly)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4) Modèle avec features polynomiales + Min-Max\n",
    "\n",
    "modele_poly = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "modele_poly.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "modele_poly.fit(X_train_poly, y_train, epochs=100, verbose=0)\n",
    "\n",
    "mae_poly = modele_poly.evaluate(X_test_poly, y_test, verbose=0)[1]\n",
    "print(\"MAE modèle polynomial =\", mae_poly)"
   ],
   "metadata": {
    "id": "tR3yLEDA-xx3"
   },
   "id": "tR3yLEDA-xx3",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e720ad48-64d1-47bc-8f8f-6e7c95312445",
   "metadata": {
    "id": "e720ad48-64d1-47bc-8f8f-6e7c95312445"
   },
   "source": [
    "### Exercise 7: Regularization\n",
    "\n",
    "1. Add L2 regularization to the multi-variable linear regression model and retrain it.\n",
    "2. Adjust the regularization strength and observe its effect on model performance and overfitting.\n",
    "3. Plot the training and validation loss curves to visualize the impact of regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 7: Regularization\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# données\n",
    "X = dataset.drop(columns=[\"MPG\"])\n",
    "y = dataset[\"MPG\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# modèle avec régularisation L2\n",
    "modele_l2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(\n",
    "        10,\n",
    "        activation=\"relu\",\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
    "    ),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "modele_l2.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "\n",
    "history_l2 = modele_l2.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"MAE (L2 = 0.01) =\", modele_l2.evaluate(X_test, y_test, verbose=0)[1])"
   ],
   "metadata": {
    "id": "7yBr189g-yxe"
   },
   "id": "7yBr189g-yxe",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_l2.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(history_l2.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "dRr24dbH_5EJ"
   },
   "id": "dRr24dbH_5EJ",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "280f364c-fd3e-475d-96d8-07caa53e3e57",
   "metadata": {
    "id": "280f364c-fd3e-475d-96d8-07caa53e3e57"
   },
   "source": [
    "### Exercise 8: Hyperparameter Tuning\n",
    "\n",
    "1. Perform hyperparameter tuning for the deep neural network model using Keras Tuner to find the optimal number of layers, units, and learning rate.\n",
    "2. Train the model with the best hyperparameters found.\n",
    "3. Compare the performance of the tuned model with the original DNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 8: Hyperparameter Tuning\n",
    "\n",
    "\n",
    "\n",
    "!pip install -q keras-tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    # nombre de couches\n",
    "    for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "        model.add(tf.keras.layers.Dense(\n",
    "            units=hp.Int(f\"units_{i}\", 32, 128, step=32),\n",
    "            activation=\"relu\"\n",
    "        ))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=hp.Choice(\"lr\", [0.001, 0.01])\n",
    "        ),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mae\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_mae\",\n",
    "    max_trials=5,\n",
    "    directory=\"tuning\",\n",
    "    project_name=\"mpg\"\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, validation_split=0.2, epochs=50, verbose=0)\n",
    "\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "print(\"MAE modèle optimisé =\", best_model.evaluate(X_test, y_test, verbose=0)[1])"
   ],
   "metadata": {
    "id": "bf5WLlXN-yfV"
   },
   "id": "bf5WLlXN-yfV",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d8c5bbd3-51f1-4b97-aff8-3503adcf55e0",
   "metadata": {
    "id": "d8c5bbd3-51f1-4b97-aff8-3503adcf55e0"
   },
   "source": [
    "### Exercise 9: Cross-Validation\n",
    "\n",
    "1. Implement k-fold cross-validation for the multi-variable linear regression model.\n",
    "2. Calculate the average mean absolute error (MAE) across all folds.\n",
    "3. Compare the cross-validation performance to the train-test split performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 9: Cross-Validation\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "maes = []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    X_tr, X_te = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_tr, y_te = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    model.fit(X_tr, y_tr, epochs=100, verbose=0)\n",
    "\n",
    "    mae = model.evaluate(X_te, y_te, verbose=0)[1]\n",
    "    maes.append(mae)\n",
    "\n",
    "print(\"MAE moyenne (cross-validation) =\", np.mean(maes))"
   ],
   "metadata": {
    "id": "VkdrjAny-zfN"
   },
   "id": "VkdrjAny-zfN",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "106e1e9e-1088-4a61-a3a6-9a8fe087dd9f",
   "metadata": {
    "id": "106e1e9e-1088-4a61-a3a6-9a8fe087dd9f"
   },
   "source": [
    "### Exercise 10: Model Deployment\n",
    "\n",
    "1. Save the trained deep neural network model to a file.\n",
    "2. Load the saved model and make predictions on a new dataset.\n",
    "3. Implement a simple Flask web application that accepts input features and returns the predicted 'MPG' value."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Exercise 10: Model Deployment (Save / Load / Predict)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1 Sauvegarder le modèle entraîné (remplace dnn_relu par ton modèle si besoin)\n",
    "dnn_relu.save(\"modele_mpg.keras\")\n",
    "\n",
    "# 2 Recharger le modèle\n",
    "modele_charge = tf.keras.models.load_model(\"modele_mpg.keras\")\n",
    "\n",
    "# 3 Nouvelles données (features) -> conversion sûre en nombres + float32\n",
    "X_new_df = dataset.drop(columns=[\"MPG\"]).iloc[:3].copy()\n",
    "X_new_df = X_new_df.apply(pd.to_numeric, errors=\"coerce\")  # force en numérique\n",
    "X_new = X_new_df.to_numpy(dtype=np.float32)               # float32 pour TF\n",
    "\n",
    "pred = modele_charge.predict(X_new, verbose=0).flatten()\n",
    "print(\"Prédictions MPG :\", pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "VqqPjeIp-z15"
   },
   "id": "VqqPjeIp-z15",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# charger le modèle sauvegardé\n",
    "modele = tf.keras.models.load_model(\"modele_mpg.keras\")\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "\n",
    "    # on attend: {\"features\": [val1, val2, ..., valN]}\n",
    "    features = request.json[\"features\"]\n",
    "    x = np.array(features, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "    y_pred = modele.predict(x, verbose=0)[0][0]\n",
    "    return jsonify({\"mpg_predicted\": float(y_pred)})\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)"
   ],
   "metadata": {
    "id": "BfKgZ9c-ABur"
   },
   "id": "BfKgZ9c-ABur",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}